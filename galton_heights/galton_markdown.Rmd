---
title: "Study on Linear Regression in Galton Heights Data"
author: "Juan Lamilla"
date: "09/04/2021"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
    keep_md: true
---

## Setup

```{r setup, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Required Libraries
library(HistData)
library(tidyverse)

# Data used
data("GaltonFamilies")


# Setup variables
galton_heights <- GaltonFamilies %>%
  filter(gender == "male") %>%
  group_by(family) %>%
  sample_n(1) %>%
  ungroup() %>%
  select(father, childHeight) %>%
  rename(son = childHeight)
```

## Introduction

This is an R Markdown document used to organize and display the code I have written for my case study in the famous 1885 study of Francis Galton exploring the relationship between the heights of adult children and the heights of their parents for my class in linear regression. 

```{r mean}
# means and standard deviations
galton_heights %>%
  summarize(mean(father), sd(father), mean(son), sd(son))
```

```{r heighs_scatter}
# scatterplot of father and son heights
galton_heights %>%
  ggplot(aes(father, son)) +
  geom_point(alpha = 0.5)
```

### Calculating Correlation Coefficient

Using the following examples, we may calculate the correlation coefficient to determine how related two variables are, conveying how they move together. The correlation coefficient is defined for a list of pairs $(x_1, y_1), \dots, (x_n,y_n)$ as the product of the standardized values, so the formula is as follows:

$$
\rho = \frac{1}{n} \sum_{i=1}^n \left( \frac{x_i-\mu_x}{\sigma_x} \right)\left( \frac{y_i-\mu_y}{\sigma_y} \right)
$$
Using this formula we can calculate the correlation coefficient between the father and sons height.

```{r CC}
# Calculating correlation coefficient
galton_heights %>% summarize(r = cor(father, son)) %>% pull(r)
```

Since correlation is calculated based on a sample of data, we may further limit the sample size to see how the correlation may be warped.

```{r SC}
# compute sample correlation
R <- sample_n(galton_heights, 25, replace = TRUE) %>%
  summarize(r = cor(father, son))
R
```

And run a monte-carlo simulation of the sample correlation:

```{r MonteCarlo}
# Monte Carlo simulation to show distribution of sample correlation
B <- 1000
N <- 25
R <- replicate(B, {
  sample_n(galton_heights, N, replace = TRUE) %>%
    summarize(r = cor(father, son)) %>%
    pull(r)
})
qplot(R, geom = "histogram", binwidth = 0.05, color = I("black"))
```

```{r mean/sd}
# expected value and standard error
mean(R)
sd(R)
```

Finally we can calculate if a sample size (N) is large enough by creating a QQ-plot

```{r QQ-Plot}
# QQ-plot to evaluate whether N is large enough
data.frame(R) %>%
  ggplot(aes(sample = R)) +
  stat_qq() +
  geom_abline(intercept = mean(R), slope = sqrt((1-mean(R)^2)/(N-2)))
```

## Stratification

To improve the estimates of the conditional expectations we can define strata with similar heights.

For example, we may want to calculate the predicted height of a son with a specific height of the fathers, like so:

```{r 72prediction}
# predicted height of a son with a 72 inch tall father
conditional_avg <- galton_heights %>%
    filter(round(father) == 72) %>%
    summarize(avg = mean(son)) %>%
    pull(avg)
conditional_avg
```

Stratification allows us to use boxplots to find the distribution of each group (between fathers/sons)

```{r 72boxplot}
# stratify fathers' heights to make a boxplot of son heights
galton_heights %>% mutate(father_strata = factor(round(father))) %>%
    ggplot(aes(father_strata, son)) +
    geom_boxplot() +
    geom_point()
```

The center of each group rises with height, and the means of each group follow a linear relationship (which we can show with the follow plot).

```{r 72center}
# center of each boxplot
galton_heights %>%
    mutate(father = round(father)) %>%
    group_by(father) %>%
    summarize(son_conditional_avg = mean(son)) %>%
    ggplot(aes(father, son_conditional_avg)) +
    geom_point()

```

#### Regression

Finally we may calculate the regression line to get a more clear understanding of the two variables.

If there is perfect correlation, the regression line predicts an increase that is the same number of SDs for both variables. If there is 0 correlation, then we don’t use x at all for the prediction and simply predict the average $μ_y$. For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase.

The formula for the regression is as follows:

$$ 
\left( \frac{Y-\mu_Y}{\sigma_Y} \right) = \rho \left( \frac{x-\mu_X}{\sigma_X} \right)
$$
or rewritten like:

$$ 
Y = \mu_Y + \rho \left( \frac{x-\mu_X}{\sigma_X} \right) \sigma_Y
$$

```{r 72regressionline}
# calculate values to plot regression line on original data
mu_x <- mean(galton_heights$father)
mu_y <- mean(galton_heights$son)
s_x <- sd(galton_heights$father)
s_y <- sd(galton_heights$son)
r <- cor(galton_heights$father, galton_heights$son)
m <- r * s_y/s_x
b <- mu_y - m*mu_x

# add regression line to plot
galton_heights %>%
    ggplot(aes(father, son)) +
    geom_point(alpha = 0.5) +
    geom_abline(intercept = b, slope = m)
```






